{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.Reading the target and type of regression to be run\n",
    "\n",
    "import json\n",
    "\n",
    "#recursive fn for iterting through each structre.\n",
    "def get_val_json(data,keys):\n",
    "  if len(keys) == 1:\n",
    "    return data.get(keys[0])\n",
    "  else:\n",
    "    new_key = keys[0]\n",
    "    if new_key in data:\n",
    "      return get_val_json(data[new_key],keys[1:])\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "# Load JSON data from the file\n",
    "with open('alogparams_from_ui1.json', 'r') as json_file:\n",
    "    j_data = json.load(json_file)\n",
    "\n",
    "#keys to be extracted\n",
    "t_type_keys = ['design_state_data', 'target', 'type']\n",
    "t_name_keys = ['design_state_data', 'target', 'target']\n",
    "\n",
    "#extract values using the generic function above\n",
    "type_val = get_val_json(j_data,t_type_keys)\n",
    "target_val = get_val_json(j_data,t_name_keys)\n",
    "\n",
    "print(\"Type:\", type_val)\n",
    "print(\"Target:\", target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.importing datasets and Reading the features \n",
    "#.. what missing imputation needs to be applied\n",
    "\n",
    "import pandas as pd\n",
    "iris_df = pd.read_csv('iris.csv')\n",
    "# print(iris_df.head())\n",
    "\n",
    "#extracting feature handling metrics from json ->j_data\n",
    "feat_hdg_info = j_data['design_state_data']['feature_handling']\n",
    "\n",
    "#loooing through features and applying missing value imputaions\n",
    "for feature_name, feature_details in feat_hdg_info.items():\n",
    "  if feature_name == 'species': #as this feature doesn't contain any Impute values\n",
    "        continue\n",
    "  if feature_details['feature_details']['missing_values'] == 'Impute':\n",
    "    impute_method = feature_details['feature_details']['impute_with']\n",
    "    impute_value = feature_details['feature_details']['impute_value']\n",
    "\n",
    "    if impute_method =='Average of values':\n",
    "      iris_df[feature_name].fillna(iris_df[feature_name].mean(), inplace=True)\n",
    "    elif impute_method == 'custom':\n",
    "      iris_df[feature_name].fillna(impute_value, inplace=True)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Computing feature reduction based on input \n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#generic function for different json files.\n",
    "def apply_featr_reduc(j_df, reduc_method, sel_features, target_feature=None):\n",
    "  if reduc_method == \"No Reduction\":\n",
    "    reduced_df = j_df[sel_features]\n",
    "\n",
    "  elif reduc_method == \"Corr with Target\":\n",
    "    corr_matrix = j_df[sel_features + [target_feature]].corr()\n",
    "    sorted_corr = corr_matrix[target_feature].abs().sort_values(ascending=False)\n",
    "    top_features = sorted_corr[1:].index.tollist()\n",
    "    reduced_df = j_df[top_features]\n",
    "\n",
    "  elif reduc_method == \"Tree-based\":\n",
    "    X = j_df[sel_features]\n",
    "    y = j_df[target_feature]\n",
    "\n",
    "    #performing one-hot encoding as this data contain string values in iris-setosa\n",
    "    X_encoded = pd.get_dummies(X)\n",
    "\n",
    "    tree_model = ExtraTreesRegressor(n_estimators=100, random_state=0)\n",
    "    tree_model.fit(X_encoded,y)\n",
    "\n",
    "    importances = tree_model.feature_importances_\n",
    "    indices = importances.argsort()[::-1]\n",
    "\n",
    "    num_top_features = min(10, len(sel_features))\n",
    "    top_indices = indices[:num_top_features]\n",
    "\n",
    "    top_features = X_encoded.columns[top_indices]\n",
    "    reduced_df = X_encoded[top_features]\n",
    "  \n",
    "  elif reduc_method == \"PCA\":\n",
    "    n_components = len(sel_features)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_df = pd.Dataframe(pca.fit_transform(j_df[sel_features]), columns=[f'PCA_{i+1}' for i in range(n_components)])\n",
    "  \n",
    "  return reduced_df\n",
    "\n",
    "#loading json file if required .. again\n",
    "with open('alogparams_from_ui1.json', 'r') as json_file:\n",
    "    j_data = json.load(json_file)\n",
    "\n",
    "#selected feature extraction\n",
    "j_data['design_state_data']['session_info']['dataset'] = 'iris.csv' #change to the json file - not using modified iris. same csv is using\n",
    "j_df = pd.read_csv(j_data['design_state_data']['session_info']['dataset'])\n",
    "sel_features = [feature_name for feature_name, feature_info in j_data['design_state_data']['feature_handling'].items() if feature_info['is_selected']]\n",
    "\n",
    "#reduction method\n",
    "reduc_method = j_data['design_state_data']['feature_reduction']['feature_reduction_method']\n",
    "\n",
    "#target features extraction\n",
    "target_feature = j_data['design_state_data']['target']['target'] if reduc_method == \"Corr with Target\" or reduc_method == \"Tree-based\" else None\n",
    "\n",
    "#APPLYING REDUCTION\n",
    "reduced_df = apply_featr_reduc(j_df, reduc_method, sel_features, target_feature)\n",
    "\n",
    "print(reduced_df)\n",
    "reduced_df.to_csv('iris_modified.csv', index=False)\n",
    "print(\"Saved reduced DataFrame to 'iris_modified.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.Parse the Json and make the model objects (using sklean) \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "with open('alogparams_from_ui1.json', 'r') as json_file:\n",
    "  j_data = json.load(json_file)\n",
    "\n",
    "#extract pred type from json\n",
    "prediction_type = j_data['design_state_data']['target']['prediction_type']\n",
    "print(prediction_type)\n",
    "\n",
    "#create a dict for mapping the type to models \n",
    "\n",
    "pred_type_model = {\n",
    "    \"Regression\":RandomForestRegressor,\n",
    "    \"Classification\":RandomForestClassifier\n",
    "    #I'm just using rfc and rfg because this json file only specifies it. add more \n",
    "}\n",
    "\n",
    "if prediction_type in pred_type_model:\n",
    "  selected_model_class = pred_type_model[prediction_type]\n",
    "  selected_model = selected_model_class()\n",
    "  print(\"Selected model : \", selected_model)\n",
    "else:\n",
    "  print(f\"prediction type not specified add new .. i commented the ex... add : {prediction_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#4.1 prediction using RandomForestRegressor()\n",
    "\n",
    "from pandas.core.arrays import categorical\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#load Json if not.. i already loaded on previous cell\n",
    "\n",
    "#load dataset from ['session_info']['dataset']\n",
    "\n",
    "target_var = j_data['design_state_data']['target']['target']\n",
    "\n",
    "features = [feature_name for feature_name, feature_info in j_data['design_state_data']['feature_handling'].items()\n",
    "            if feature_name != target_var and feature_info['is_selected']]\n",
    "\n",
    "#converting categ variable to onehot encoded format to convert strings to float\n",
    "\n",
    "categorical_columns = [feature_name for feature_name, feature_info in j_data['design_state_data']['feature_handling'].items()\n",
    "                        if feature_name != target_var and feature_info['feature_variable_type'] == 'text']\n",
    "iris_df = pd.get_dummies(iris_df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# iris_df = iris_df.drop(columns=['species'])\n",
    "X = iris_df[features]\n",
    "y = iris_df[target_var]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#extract rfg from json\n",
    "rf_regressor_config = j_data['design_state_data']['algorithms']['RandomForestRegressor']\n",
    "\n",
    "#i removed these params from config as rgc doesn't require , add or remove with additional models \n",
    "invalid_params = ['model_name', 'is_selected', 'min_trees', 'max_trees', 'feature_sampling_statergy','min_depth',\n",
    "                  'min_samples_per_leaf_min_value','min_samples_per_leaf_max_value','parallelism']\n",
    "for param in invalid_params:\n",
    "    rf_regressor_config.pop(param, None)\n",
    "\n",
    "selected_model = RandomForestRegressor(**rf_regressor_config) #instantiate using rfg from json\n",
    "\n",
    "selected_model.fit(X_train, y_train) #ftting and prediction\n",
    "\n",
    "y_pred = selected_model.predict(X_test)\n",
    "\n",
    "#calculating mse\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "print(\"mean_squared_error: \",mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.Run the fit and predict on each model\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "#first create a dictionary as the json and df are already loaded in above cells.\n",
    "\n",
    "model_info = {\n",
    "    \"RandomForestRegressor\": {\"model_class\": RandomForestRegressor,\"eval_metric\":mean_squared_error},\n",
    "    \"RandomForestClassifier\":{\"model_class\": RandomForestClassifier,\"eval_metric\":accuracy_score},\n",
    "}\n",
    "\n",
    "invalid_params = ['model_name', 'is_selected', 'min_trees', 'max_trees', 'feature_sampling_statergy', 'min_depth',\n",
    "                  'min_samples_per_leaf_min_value', 'min_samples_per_leaf_max_value', 'parallelism']\n",
    "\n",
    "for model_name,model_config in j_data['design_state_data']['algorithms'].items():\n",
    "  if model_name not in model_info:\n",
    "    continue \n",
    "\n",
    "  #extract hyperpram from json and remove the others\n",
    "  hyperparams = {param: value for param, value in model_config.items() if param not in invalid_params}\n",
    "\n",
    "  model_class = model_info[model_name]['model_class']\n",
    "\n",
    "  hyperparam_grid = {\n",
    "      param: [value] if isinstance(value, (int, float)) else value\n",
    "      for param, value in hyperparams.items()\n",
    "  }\n",
    "\n",
    "\n",
    "  grid_search = GridSearchCV(model_class(), hyperparam_grid, cv=5, scoring='neg_mean_squared_error', error_score='raise')\n",
    "  try:\n",
    "    grid_search.fit(X_train, y_train)  # Performing tuning using GridSearchCV\n",
    "  except Exception as e:\n",
    "    print(f\"Fit failed for {model_name} - Exception: {e}\")\n",
    "\n",
    "  best_model = grid_search.best_estimator_\n",
    "  best_model.fit(X_train,y_train)\n",
    "\n",
    "  y_pred = best_model.predict(X_test) #making prediction using best fit model\n",
    "\n",
    "  eval_metric = model_info[model_name]['eval_metric']\n",
    "  score = eval_metric(y_test, y_pred)\n",
    "\n",
    "  print(f\"{model_name} - Evaluation Metrics : {score}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
