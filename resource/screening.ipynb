{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#recursive fn for iterting through each structre.\n",
    "def get_val_json(data,keys):\n",
    "  if len(keys) == 1:\n",
    "    return data.get(keys[0])\n",
    "  else:\n",
    "    new_key = keys[0]\n",
    "    if new_key in data:\n",
    "      return get_val_json(data[new_key],keys[1:])\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "# Load JSON data from the file\n",
    "with open('alogparams_from_ui1.json', 'r') as json_file:\n",
    "    j_data = json.load(json_file)\n",
    "\n",
    "#keys to be extracted\n",
    "t_type_keys = ['design_state_data', 'target', 'type']\n",
    "t_name_keys = ['design_state_data', 'target', 'target']\n",
    "\n",
    "#extract values using the generic function above\n",
    "type_val = get_val_json(data,t_type_keys)\n",
    "target_val = get_val_json(data,t_name_keys)\n",
    "\n",
    "print(\"Type:\", type_val)\n",
    "print(\"Target:\", target_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.importing datasets and Reading the features \n",
    "#.. what missing imputation needs to be applied\n",
    "\n",
    "import pandas as pd\n",
    "iris_df = pd.read_csv('iris.csv')\n",
    "# print(iris_df.head())\n",
    "\n",
    "#extracting feature handling metrics from json ->j_data\n",
    "feat_hdg_info = j_data['design_state_data']['feature_handling']\n",
    "\n",
    "#loooing through features and applying missing value imputaions\n",
    "for feature_name, feature_details in feat_hdg_info.items():\n",
    "  if feature_name == 'species': #as this feature doesn't contain any Impute values\n",
    "        continue\n",
    "  if feature_details['feature_details']['missing_values'] == 'Impute':\n",
    "    impute_method = feature_details['feature_details']['impute_with']\n",
    "    impute_value = feature_details['feature_details']['impute_value']\n",
    "\n",
    "    if impute_method =='Average of values':\n",
    "      iris_df[feature_name].fillna(iris_df[feature_name].mean(), inplace=True)\n",
    "    elif impute_method == 'custom':\n",
    "      iris_df[feature_name].fillna(impute_value, inplace=True)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.Computing feature reduction based on input \n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#generic function for different json files.\n",
    "def apply_featr_reduc(j_df, reduc_method, sel_features, target_feature=None):\n",
    "  if reduc_method == \"No Reduction\":\n",
    "    reduced_df = j_df[sel_features]\n",
    "\n",
    "  elif reduc_method == \"Corr with Target\":\n",
    "    corr_matrix = j_df[sel_features + [target_feature]].corr()\n",
    "    sorted_corr = corr_matrix[target_feature].abs().sort_values(ascending=False)\n",
    "    top_features = sorted_corr[1:].index.tollist()\n",
    "    reduced_df = j_df[top_features]\n",
    "\n",
    "  elif reduc_method == \"Tree-based\":\n",
    "    X = j_df[sel_features]\n",
    "    y = j_df[target_feature]\n",
    "\n",
    "    #performing one-hot encoding as this data contain string values in iris-setosa\n",
    "    X_encoded = pd.get_dummies(X)\n",
    "\n",
    "    tree_model = ExtraTreesRegressor(n_estimators=100, random_state=0)\n",
    "    tree_model.fit(X_encoded,y)\n",
    "\n",
    "    importances = tree_model.feature_importances_\n",
    "    indices = importances.argsort()[::-1]\n",
    "\n",
    "    num_top_features = min(10, len(sel_features))\n",
    "    top_indices = indices[:num_top_features]\n",
    "\n",
    "    top_features = X_encoded.columns[top_indices]\n",
    "    reduced_df = X_encoded[top_features]\n",
    "  \n",
    "  elif reduc_method == \"PCA\":\n",
    "    n_components = len(sel_features)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_df = pd.Dataframe(pca.fit_transform(j_df[sel_features]), columns=[f'PCA_{i+1}' for i in range(n_components)])\n",
    "  \n",
    "  return reduced_df\n",
    "\n",
    "#loading json file if required .. again\n",
    "with open('alogparams_from_ui1.json', 'r') as json_file:\n",
    "    j_data = json.load(json_file)\n",
    "\n",
    "#selected feature extraction\n",
    "j_data['design_state_data']['session_info']['dataset'] = 'iris.csv' #change to the json file - not using modified iris. same csv is using\n",
    "j_df = pd.read_csv(j_data['design_state_data']['session_info']['dataset'])\n",
    "sel_features = [feature_name for feature_name, feature_info in j_data['design_state_data']['feature_handling'].items() if feature_info['is_selected']]\n",
    "\n",
    "#reduction method\n",
    "reduc_method = j_data['design_state_data']['feature_reduction']['feature_reduction_method']\n",
    "\n",
    "#target features extraction\n",
    "target_feature = j_data['design_state_data']['target']['target'] if reduc_method == \"Corr with Target\" or reduc_method == \"Tree-based\" else None\n",
    "\n",
    "#APPLYING REDUCTION\n",
    "reduced_df = apply_featr_reduc(j_df, reduc_method, sel_features, target_feature)\n",
    "\n",
    "print(reduced_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
